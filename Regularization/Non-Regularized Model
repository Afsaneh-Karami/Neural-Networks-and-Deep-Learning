def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1, initialization = "he"):
 """
 inputs :
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)
    learning_rate -- learning rate of the optimization
    num_iterations -- number of iterations of the optimization loop
    print_cost -- If True, print the cost every 10000 iterations
    lambd -- regularization hyperparameter, scalar
    keep_prob - probability of keeping a neuron active during drop-out, scalar.
    initialization -- flag to choose which initialization to use ("zeros","random" or "he")
    
Returns:
    parameters -- parameters learned by the model. They can then be used to predict.
"""

grads = {}
costs = []                            # to keep track of the cost
m = X.shape[1]                        # number of examples
layers_dims = [X.shape[0], 20, 3, 1]
    
# Initialize parameters dictionary.
if initialization == "zeros":
    parameters = initialize_parameters_zeros(layers_dims)
elif initialization == "random":
    parameters = initialize_parameters_random(layers_dims)
elif initialization == "he":
    parameters = initialize_parameters_he(layers_dims)
    
# Loop (gradient descent)
for i in range(0, num_iterations):

    # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
    if keep_prob == 1:
            a3, cache = forward_propagation(X, parameters)
    elif keep_prob < 1:
            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)
        
    # Cost function
    if lambd == 0:
            cost = compute_cost(a3, Y)
    else:
            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)
            
    # Backward propagation.
    # it is possible to use both L2 regularization and dropout, but we explore just one of them at a time                                        
    if lambd == 0 and keep_prob == 1:
            grads = backward_propagation(X, Y, cache)
    elif lambd != 0:
            grads = backward_propagation_with_regularization(X, Y, cache, lambd)
    elif keep_prob < 1:
            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)
        
    # Update parameters.
    parameters = update_parameters(parameters, grads, learning_rate)
        
    # Print the loss every 10000 iterations
    if print_cost and i % 10000 == 0:
            print("Cost after iteration {}: {}".format(i, cost))
    if print_cost and i % 1000 == 0:
            costs.append(cost)
    
# plot the cost
plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (x1,000)')
plt.title("Learning rate =" + str(learning_rate))
plt.show()
    
    return parameters

    
