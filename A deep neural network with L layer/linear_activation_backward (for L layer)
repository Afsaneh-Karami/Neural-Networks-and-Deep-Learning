def L_model_backward(AL, Y, caches):
grads = {}
L = len(caches)                                            # the number of layers (note: input layer is not counted)
m = AL.shape[1]
Y = Y.reshape(AL.shape)                                    # after this line, Y is the same shape as AL
dAL =- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))       # for last layer: dAL=-Y/AL+(1-Y)/(1-AL)
current_cache =caches[L-1]
dA_prev_temp, dW_temp, db_temp =linear_activation_backward(dAL, current_cache, activation='sigmoid')
grads["dA" + str(L-1)] =dA_prev_temp
grads["dW" + str(L)] =dW_temp
grads["db" + str(L)] =db_temp
 
for l in reversed(range(L-1)):                              # Loop from l=L-2 to l=0
        current_cache =caches[l]
        dA_prev_temp, dW_temp, db_temp =linear_activation_backward(grads["dA" + str(l + 1)], current_cache, activation='relu')
        grads["dA" + str(l)] =dA_prev_temp
        grads["dW" + str(l + 1)] =dW_temp
        grads["db" + str(l + 1)] =db_temp

return grads
