def L_model_backward(AL, Y, caches):
grads = {}
L = len(caches)                    # the number of layers (note: input layer is not counted)
m = AL.shape[1]
Y = Y.reshape(AL.shape)            # after this line, Y is the same shape as AL
dAL =- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))     # for last layer: dAL=-Y/AL+(1-Y)/(1-AL)
current_cache =caches[L-1]
dA_prev_temp, dW_temp, db_temp =linear_activation_backward(dAL, current_cache, activation='sigmoid')
