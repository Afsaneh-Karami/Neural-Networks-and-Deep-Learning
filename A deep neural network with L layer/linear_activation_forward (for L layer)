def L_model_forward(X, parameters):
caches = []                               # we choose list data Type because it can be appended
A = X
L = len(parameters) // 2                  # number of layers in the neural network (Note:Input layer is not counted)
for l in range(1, L):                     # Loop for L=1,2,...,L-1
     A_prev = A 
     A, cache = linear_activation_forward (A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation="relu")
     caches.append(cache)                 
AL, cache =linear_activation_forward (A, parameters['W' + str(L)], parameters['b' + str(L)], activation="sigmoid")
caches.append(cache)                   # caches=[((A1,W1,b1),'relu'),((A2,W2,b2),'relu'),...,((AL,WL,bL),'sigmoid')]
return AL, caches
