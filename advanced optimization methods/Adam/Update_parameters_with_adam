def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate,beta1, beta2,  epsilon):
                                
    """
    Update parameters using Adam
    
    Arguments:
    parameters -- python dictionary containing your parameters:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients for each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    v -- Adam variable, moving average of the first gradient, python dictionary
    s -- Adam variable, moving average of the squared gradient, python dictionary
    t -- Adam variable, counts the number of taken steps
    learning_rate -- the learning rate, scalar.
    beta1 -- Exponential decay hyperparameter for the first moment estimates 
    beta2 -- Exponential decay hyperparameter for the second moment estimates 
    epsilon -- hyperparameter preventing division by zero in Adam updates

    Returns:
    parameters -- python dictionary containing your updated parameters 
    v -- Adam variable, moving average of the first gradient, python dictionary
    s -- Adam variable, moving average of the squared gradient, python dictionary
    """
    
    L = len(parameters) // 2                 # number of layers in the neural networks
    v_corrected = {}                         # Initializing first moment estimate, python dictionary
    s_corrected = {}                         # Initializing second moment estimate, python dictionary
    
       for l in range(1, L + 1):
        v["dW" + str(l)] =beta1 * v["dW" + str(l)] + (1-beta1)*grads['dW'+str(l)]
        v["db" + str(l)] =beta1 * v["db" + str(l)] + (1-beta1)*grads['db'+str(l)]
        
        v_corrected["dW" + str(l)] =v["dW" + str(l)]/(1-beta1**t)
        v_corrected["db" + str(l)] =v["db" + str(l)]/(1-beta1**t)
        
        s["dW" + str(l)] = beta2 * s["dW" + str(l)] + (1-beta2) * np.square(grads['dW'+str(l)])
        s["db" + str(l)] = beta2 * s["db" + str(l)] + (1-beta2) * np.square(grads['db'+str(l)])
        
        s_corrected["dW" + str(l)] =s["dW" + str(l)]/(1-beta2**t)
        s_corrected["db" + str(l)] =s["db" + str(l)]/(1-beta2**t)
        
        parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate *v_corrected["dW" + str(l)]/(np.sqrt(s_corrected["dW" + str(l)])+epsilon)
        parameters["b" + str(l)] = parameters["b" + str(l)]- learning_rate *v_corrected["db" + str(l)]/(np.sqrt(s_corrected["db" + str(l)])+epsilon)
       
    return parameters, v, s, v_corrected, s_corrected
